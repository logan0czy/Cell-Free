{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1fur9ClgfFi2pdmIkj360ANAexKMQw2Wz","authorship_tag":"ABX9TyOdxzZdjIOf7VnomG9bRlkf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"oB0hRVsnYJM-"},"source":["# import os\n","# os.chdir('/content/drive/My Drive/Projects/cell-free-run')\n","\n","# import numpy as np\n","# from utils import Environment\n","# power = 30\n","# env = Environment(10**(power/10))\n","# np.random.seed(1949)\n","# G_valid, F_valid, H_valid = [], [], []\n","# for i in range(501):\n","#   G, F, H = env.getCSI()\n","#   G_valid.append(G)\n","#   F_valid.append(F)\n","#   H_valid.append(H)\n","# G_valid = np.stack(G_valid)\n","# F_valid = np.stack(F_valid)\n","# H_valid = np.stack(H_valid)\n","\n","# path = \"./data\"\n","# np.save(path+\"/G_%s\"%(str(power)), G_valid)\n","# np.save(path+\"/F_%s\"%(str(power)), F_valid)\n","# np.save(path+\"/H_%s\"%(str(power)), H_valid)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXfS05_FzYng"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kU9G2zltYNB9","executionInfo":{"status":"ok","timestamp":1600787930969,"user_tz":-480,"elapsed":1853,"user":{"displayName":"Zhiyu Cao","photoUrl":"","userId":"06432530406328729087"}},"outputId":"71c86641-9242-43e2-b918-ec04735d972f","colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["import os\n","os.chdir('/content/drive/My Drive/Projects/cell-free-run')\n","import time\n","import random\n","import math\n","\n","from IPython.display import clear_output\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F \n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","%config InlineBackend.figure_format='retina'\n","%matplotlib inline\n","import seaborn as sns\n","sns.set()\n","\n","from utils import Environment, Memory\n","from net import TD3, OUStrategy\n","\n","\n","def explore(csi, state, timestep):\n","    \"\"\"explore the environment and record transitions.\n","    Parameters:\n","    csi : current CSI\n","    state : current state\n","    timestep : the current timestep\n","    \"\"\"\n","    model.eval()\n","    # get action from policy\n","    raw_act1, raw_act2 = model.actor(torch.tensor(state, dtype=torch.float, device=model.device).unsqueeze(0))\n","    act1, act2 = OUnoise.getActFromRaw(raw_act1, raw_act2, timestep)\n","    act2 = model.act2Trans(act2, env)\n","    # compute the average rate\n","    act1, act2 = act1.squeeze(0).detach().cpu().numpy(), act2.squeeze(0).detach().cpu().numpy()\n","    avg_rate = env.getRate(csi, act1, act2)\n","    csi_next = env.getCSI()\n","    state_next = env.getState(csi_next, None, None)\n","    # store the experience\n","    memory.push(state, np.concatenate((act1, act2)), avg_rate, state_next)\n","    return csi_next, state_next, avg_rate\n","\n","def validation(G_total, F_total, H_total, act1_init=None, act2_init=None):\n","    \"\"\"validate the performance of current model.\n","    \"\"\"\n","    model.eval()\n","    n_bs, n_ris, n_user = env.getCount()\n","    power, rate = 0, 0\n","    i = 0\n","    for G, F, H in zip(G_total, F_total, H_total):\n","        state = env.getState((G, F, H), None, None)\n","        act1, act2 = model.actor(torch.tensor(state, dtype=torch.float, device=model.device).unsqueeze(0))\n","        act2 = model.act2Trans(act2, env)\n","        act1, act2 = act1.squeeze(0).detach().cpu().numpy(), act2.squeeze(0).detach().cpu().numpy()\n","        avg_rate = env.getRate((G, F, H), act1, act2)\n","        rate += avg_rate\n","        i += 1    \n","        # check the output beamforming and reflecting matrix\n","        W, Phi = env.actTrans(act1, act2)\n","        W = W.reshape(n_bs, n_user, 2, env.M)\n","        power = max(power, np.amax(np.sum(W**2, (1, 2, 3))))\n","        if i==100 or i==200:\n","            print('validate action\\n', act1[:5], '\\n', act2[:5])\n","    print(\"maximum used power:\", power) \n","    return rate/i, power\n","\n","def save_opt(fpath):\n","    \"\"\"save optimizer\"\"\"\n","    state_dicts = {'opt_act': opt_actor.state_dict(), 'opt_critic': opt_critic.state_dict()}\n","    torch.save(state_dicts, fpath+'/optim.bin')\n","\n","def load_opt(fpath):\n","    \"\"\"load optimizer\"\"\"\n","    state_dicts = torch.load(fpath+'/optim.bin', map_location=lambda storage, loc: storage)\n","    opt_actor.load_state_dict(state_dicts['opt_act'])\n","    opt_critic.load_state_dict(state_dicts['opt_critic'])\n","\n","def numTrans(number):\n","    number = abs(number)\n","    if number//1 == 0:\n","        return 1\n","    if number//10 == 0:\n","        return math.floor(number)\n","    return (number//10)*10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZReVIayYRyU"},"source":["def train(epoches, steps, dBm, out_path='./checkpoint', **args):\n","  global env, model, memory, OUnoise, OUnoise_target, opt_critic, opt_actor\n","  # -------- environment --------\n","  env = Environment(10**(dBm/10))\n","  print(\"environment initialized...\")\n","\n","  # -------- buffer --------\n","  memory = Memory(int(1e4))\n","  print(\"buffer initialized...\")\n","\n","  # -------- validation data --------\n","  G_valid = np.load(\"./data/G_%s.npy\"%(str(dBm)))\n","  F_valid = np.load(\"./data/F_%s.npy\"%(str(dBm)))\n","  H_valid = np.load(\"./data/H_%s.npy\"%(str(dBm)))\n","\n","  # ---- model hyperparameters ----\n","  torch.manual_seed(2020)\n","  random.seed(2020)\n","  batch_size = 64\n","  reward_decay = 0.1\n","  policy_upfreq = 2\n","  sync_rate = 0.001\n","  lr_decay = 0.999\n","  grad_clip = 10\n","  lr_c = args['lr_critic']\n","  lr_a = args['lr_actor']\n","  c_h_size = args['critic_hidden_size']\n","  a_h_size = args['actor_hidden_size']\n","  n_c_hidden = args['n_critic_hidden']\n","  n_a_hidden = args['n_actor_hidden']\n","  c_w_decay = args['critic_weight_decay']\n","  a_w_decay = args['actor_weight_decay']\n","  \n","  # -------- create model --------\n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","  torch.backends.cudnn.benchmark = True\n","  model = TD3(env.state_size,\n","         (env.act_space['dim1'], env.act_space['dim2']),\n","         a_h_size,\n","         c_h_size, \n","         n_c_hidden, \n","         n_a_hidden,\n","         max_act=(env.act_space['bound1'], env.act_space['bound2']))\n","  model.to(device)\n","  model.init()\n","  # model.load(out_path)\n","  \n","  opt_critic = torch.optim.Adam(model.critic.parameters(),\n","                  weight_decay=c_w_decay,\n","                  lr=lr_c)\n","  opt_actor = torch.optim.Adam(model.actor.parameters(),\n","                  weight_decay=a_w_decay,\n","                  lr=lr_a)\n","  # load_opt(out_path)\n","  # lr_c = opt_critic.param_groups[0]['lr']\n","  # lr_a = opt_actor.param_groups[0]['lr']\n","\n","  # -------- noise --------\n","  OUnoise = OUStrategy(model.device, env.act_space, max_sigma=(1/100, 1/20))\n","  OUnoise_target = OUStrategy(model.device, env.act_space, theta=1, max_sigma=(1/200, 1/40), noise_bound=(1/100, 1/20))\n","  print(\"noise initialized...\")\n","\n","  # -------- explore --------\n","  with torch.no_grad():\n","    csi_, state_ = env.reset()\n","    for t in tqdm(range(1000), desc='exploring'):\n","      csi_, state_, _ = explore(csi_, state_, t)\n","  \n","  # -------- training --------\n","  info_interval = 100\n","  validate_interval = 1000\n","  start_time, train_time = time.time(), time.time()\n","  iter_n, critic_count, trial = 0, 0, 0\n","  best_score = 0.\n","  print(\"now begin to train networks ......\")\n","\n","  csi_explore, state_explore = env.reset()\n","  for epoch in range(epoches):\n","    if epoch % 5 == 0: clear_output()\n","    rate_info, loss_c_info, loss_a_info, q_info = [], [], [], []\n","    OUnoise.reset()\n","    OUnoise_target.reset()\n","\n","    for step in range(steps):\n","      iter_n += 1\n","      # ---- explore the environment ----\n","      with torch.no_grad():\n","        csi_explore, state_explore, rate_explore = explore(csi_explore, state_explore, step)\n","      rate_info.append(rate_explore)\n","          \n","      # ---- get batch transition memory ----\n","      state, action, reward, state_next = memory.getBatch(batch_size)\n","      state = torch.tensor(state, dtype=torch.float, device=model.device)\n","      action = torch.tensor(action, dtype=torch.float, device=model.device)\n","      reward = torch.tensor(reward, dtype=torch.float, device=model.device).view(-1, 1)\n","      state_next = torch.tensor(state_next, dtype=torch.float, device=model.device)\n","  \n","      # ---- update critic networks ----\n","      model.actor_target.eval()\n","      model.critic_target.eval()\n","      act1_next, act2_next = model.actor_target(state_next)\n","      act1_next, act2_next = OUnoise_target.getActFromRaw(act1_next, act2_next, step)\n","      act2_next = model.act2Trans(act2_next, env)\n","      act_next = torch.cat((act1_next, act2_next), dim=1)\n","      Q_prime = reward + reward_decay*model.critic_target(state_next, act_next).detach()\n","  \n","      model.critic.train()\n","      Q1 = model.critic.critic1(state, action)\n","      Q2 = model.critic.critic2(state, action)\n","      loss_c = F.mse_loss(Q1, Q_prime) + F.mse_loss(Q2, Q_prime)\n","      loss_c_info.append(loss_c.item())\n","      \n","      opt_critic.zero_grad()\n","      loss_c.backward()\n","      nn.utils.clip_grad_norm_(model.critic.critic1.parameters(), grad_clip)\n","      nn.utils.clip_grad_norm_(model.critic.critic2.parameters(), grad_clip)\n","      opt_critic.step()\n","  \n","      critic_count += 1\n","      if critic_count % policy_upfreq == 0:\n","        # ---- update actor networks ----\n","        critic_count = 0\n","        model.actor.train()\n","        # get action from policy\n","        act1_eager, act2_eager = model.actor(state)\n","        act2_eager = model.act2Trans(act2_eager, env)\n","        act_eager = torch.cat((act1_eager, act2_eager), 1)\n","        # calculate punishment\n","        punish1 = model.act1Check(act1_eager, env)\n","        # punish2 = torch.sum(model.actor.output1.weight**2) + torch.sum(model.actor.output2.weight**2)\n","\n","        # calculate loss\n","        model.critic.eval()\n","        Q_achieve = model.critic.getQ(state, act_eager).mean()\n","        # scale_num = numTrans(Q_achieve.item())\n","        loss_a = -1.0 * (Q_achieve - 1 * punish1) \n","        q_info.append(Q_achieve.item())\n","        loss_a_info.append(loss_a.item())\n","   \n","        opt_actor.zero_grad()\n","        loss_a.backward()\n","        nn.utils.clip_grad_norm_(model.actor.parameters(), grad_clip)\n","        opt_actor.step()\n","  \n","        # ---- synchronize network ----\n","        with torch.no_grad():\n","          model.sync(sync_rate)\n","  \n","      # ---- loss information ----\n","      if iter_n % info_interval == 0:\n","        print(\"epoch: %d, iter: %d, best: %.4f, avg.rate: %.4f, loss_c: %.4f, loss_a: %.2f, Q: %.2f, speed: %.2f/iter, time eclapsed: %dsec\"\n","          % (epoch,\n","            iter_n,\n","            best_score,\n","            np.mean(rate_info[-info_interval:]),\n","            np.mean(loss_c_info[-info_interval:]),\n","            np.mean(loss_a_info[-info_interval:]),\n","            np.mean(q_info[-info_interval:]),\n","            (time.time()-train_time)/info_interval,\n","            time.time()-start_time)\n","        )\n","        train_time = time.time()\n","\n","      # ---- validation ----\n","      if iter_n % validate_interval == 0:\n","        print(\"validation begins, check whether the model upgraded......\")\n","        avg_rate, power_used = validation(G_valid, F_valid, H_valid)\n","        print(\"----> avg.rate: %.4f\" % avg_rate)\n","\n","        if epoch<5:\n","          continue\n","  \n","        if avg_rate > best_score and power_used<=env.power_max:\n","          print(\"model upgraded...save to folder %s\"%out_path)\n","          trial = 0\n","          best_score = avg_rate\n","          save_opt(out_path)\n","          model.save(out_path)\n","        else:\n","          trial += 1\n","          print(\"hit trial %d\"%trial)\n","\n","          lr_c = max(1e-4, lr_c*lr_decay)\n","          print(\"lr_critic change to %.4e\"%lr_c)\n","          for param in opt_critic.param_groups:\n","            param['lr'] = lr_c\n","\n","          lr_a = max(1e-4, lr_a*lr_decay)\n","          print(\"lr_actor change to %.4e\"%lr_a)\n","          for param in opt_actor.param_groups:\n","            param['lr'] = lr_a\n","          \n","          # if trial%20 == 0:\n","          #   print(\"reload model\")\n","          #   model.load(out_path)\n","          #   load_opt(out_path)\n","          #   lr_c = opt_critic.param_groups[0]['lr']\n","          #   lr_a = opt_actor.param_groups[0]['lr']\n","        \n","  return best_score, trial"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUsx7MfEYUUM","executionInfo":{"status":"error","timestamp":1600814261192,"user_tz":-480,"elapsed":26324975,"user":{"displayName":"Zhiyu Cao","photoUrl":"","userId":"06432530406328729087"}},"outputId":"7ae3064b-49bf-4d99-e152-6669f1828f2b","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["epoches, steps = 500, int(5e3)\n","args = {'lr_critic':3e-4, 'lr_actor':3e-4, 'critic_hidden_size': -1, 'actor_hidden_size': -1,\n","      'n_critic_hidden': -1, 'n_actor_hidden': -1, 'critic_weight_decay': 0, 'actor_weight_decay': 0}\n","score, trial = train(epoches, steps, 30, **args)\n","print('\\n\\n', score, trial, loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWlUl8dE8aWw"},"source":["env = Environment(10**(30/10))\n","env.action_space['bound'] = 2\n","print(\"environment initialized...\")\n","\n","# -------- create model --------\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","torch.backends.cudnn.benchmark = True\n","model = TD3(env.state_size,\n","       env.action_space['dim1'],\n","       env.action_space['dim2'],\n","       2048,\n","       2048, \n","       2, \n","       3,\n","       bound=env.action_space['bound'])\n","model.to(device)\n","model.init()\n","model.load('./checkpoint')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpwO2owc9CUH"},"source":["print(model.actor.bn_out2.running_mean)\n","print(model.actor.bn_out2.running_var)"],"execution_count":null,"outputs":[]}]}